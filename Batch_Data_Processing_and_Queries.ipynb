{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2046100-5a56-4d83-9a16-9212d7567774",
     "showTitle": true,
     "title": "Check contents in FileStore"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check the contents in FileStore\n",
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f0440b9-d50c-49fc-aaa0-f06ec74e2cec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Mount S3 bucket to Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5dd5fc-32b9-4f05-b99b-85e47d171132",
     "showTitle": true,
     "title": "Step 1: Loading packages "
    }
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a814fed-a3f0-492a-a85a-4cf2565c40da",
     "showTitle": true,
     "title": "Step 2: Read the csv file containing the AWS keys to Databricks "
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47421eb-097d-41f8-beb1-c727278a4e19",
     "showTitle": true,
     "title": "Step 3: Extract the access key and secret access key from the spark dataframe created above"
    }
   },
   "outputs": [],
   "source": [
    "# Notes: the secret access key will be encoded using urllib.parse.quote for security purposes. \n",
    "# safe=\"\" means that every character will be encoded.\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "#ACCESS_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Access key ID').collect()[0]['Access key ID']\n",
    "#SECRET_KEY = aws_keys_df.where(col('User name')=='databricks-user').select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e03419-e3e2-4132-be03-0f56bee9c6a1",
     "showTitle": true,
     "title": "Step 4: Mount the S3 bucket"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# mount the S3 bucket by passing in the S3 URL and the desired mount name to dbutils.fs.mount()\n",
    "\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-0a5e6ec37a2f-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/pin_data\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)\n",
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f016d2e6-b3e1-479c-b919-06c88c155ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9edef9a0-f275-42d6-8d2b-26243c820bab",
     "showTitle": true,
     "title": "Dataframe 1: Pinterset"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data about category, description, downloaded, follower_count, image_src\n",
    "# # File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pin_data/topics/0a5e6ec37a2f.pin/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_pin_raw = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_pin_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3568bbab-14e4-4405-b95a-7ede686a2520",
     "showTitle": true,
     "title": "Dataframe 2: Geolocation"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data about country, latitude, longtitude\n",
    "# # File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pin_data/topics/0a5e6ec37a2f.geo/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo_raw = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_geo_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67c2892-ed02-45ee-8f94-c44110e4ec42",
     "showTitle": true,
     "title": "Dataframe 3: User"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data about age, date_joined, first_name, ind, last_name\n",
    "# # File location and type\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = \"/mnt/pin_data/topics/0a5e6ec37a2f.user/partition=0/*.json\" \n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user_raw = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "display(df_user_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54ac0203-a802-4cf9-9d15-cc5657ef56fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31c6d585-67ad-4884-84fd-68157dc82464",
     "showTitle": true,
     "title": "Cleaning pinterest dataframe (df_pin_raw)"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Drop duplicates\n",
    "df_pin = df_pin_raw.dropDuplicates()\n",
    "\n",
    "# Handle empty entires and entries with error\n",
    "# Replace them with \"Nones\"\n",
    "df_pin = df_pin.replace(['', ' ', 'NULL', 'null'], [None] * 4)\n",
    "df_pin = df_pin.withColumnRenamed('index', 'ind')\n",
    "df_pin = df_pin.withColumn(\"description\", when(col(\"description\") == \"No description available Story format\", None).otherwise(col(\"description\")))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", when(col(\"follower_count\") == \"User Info Error\", None).otherwise(col(\"follower_count\")))\n",
    "df_pin = df_pin.withColumn(\"image_src\", when(col(\"image_src\") == \"Image src error.\", None).otherwise(col(\"image_src\")))\n",
    "df_pin = df_pin.withColumn(\"poster_name\", when(col(\"poster_name\") == \"User Info Error\", None).otherwise(col(\"poster_name\")))\n",
    "df_pin = df_pin.withColumn(\"tag_list\", when(col(\"tag_list\") == \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", None).otherwise(col(\"tag_list\")))\n",
    "df_pin = df_pin.withColumn(\"title\", when(col(\"title\") == \"No Title Data Available\", None).otherwise(col(\"title\")))\n",
    "\n",
    "# Transform follower_count to ensure every entry is a number\n",
    "# remove non-numeric entries, such that data type is \"int\"\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(col(\"follower_count\"), \"[^0-9]\", \"\"))\n",
    "df_pin = df_pin.withColumn(\"follower_count\", col(\"follower_count\").cast(IntegerType()))\n",
    "\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "df_pin = df_pin.withColumn(\"downloaded\", col(\"downloaded\").cast(IntegerType()))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"Local save in \", \"\"))\n",
    "\n",
    "# Rename the index column to ind\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_pin = df_pin.select([\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"])\n",
    "\n",
    "display(df_pin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9df8ea-7d07-4532-813e-30c6adcf0ac2",
     "showTitle": true,
     "title": "Cleaning geolocation dataframe (df_geo_raw)"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_geo = df_geo_raw.dropDuplicates()\n",
    "\n",
    "# Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_geo = df_geo.select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "\n",
    "display(df_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f24e3a8-d4ec-43f6-a04e-78a336a3bb3e",
     "showTitle": true,
     "title": "Cleaning user dataframe (df_user_raw)"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_user = df_user_raw.dropDuplicates()\n",
    "\n",
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_user = df_user.select([\"ind\", \"user_name\", \"age\", \"date_joined\"])\n",
    "\n",
    "display(df_user)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd2a94d3-8f49-4a36-a290-e380429ff1a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Query using Spark on Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd9fae6-3204-4e4d-beee-c2c2cf001c43",
     "showTitle": true,
     "title": "Query 1) Identify the most popular Pinterest category people post to, based on their country"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Join geo and pin dataframes, on the 'ind' (index) column\n",
    "df_pin_geo = df_geo.join(df_pin, \"ind\")\n",
    "\n",
    "# Group by country and category, then count the frequency\n",
    "df_category_count = df_pin_geo.groupBy([\"country\", \"category\"]).agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Find and extract the most popular category for each country (by sorting within each group)\n",
    "window = Window.partitionBy(\"country\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "df_most_popular = df_category_count.withColumn(\"rank\", rank().over(window)) \\\n",
    "                                   .filter(col(\"rank\") == 1) \\\n",
    "                                   .drop(\"rank\")\n",
    "\n",
    "#The final dataframe with country, pinterest category, and category count\n",
    "df_category_final = df_most_popular.select(\"country\", \"category\", \"category_count\")\n",
    "\n",
    "display(df_category_final.select(\"*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f445654-164b-4b10-a2f2-ea303cbf3f58",
     "showTitle": true,
     "title": "Query 2) Identify the number of posts each category had (2018-2022)"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_pin_geo = df_pin.join(df_geo, 'ind', 'inner')\n",
    "\n",
    "# Convert the timestamp column from string to timestamp type if it's not already\n",
    "df_pin_geo = df_pin_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Filter the DataFrame for posts between 2018 and 2022\n",
    "df_5years = df_pin_geo.filter((year(\"timestamp\") >= 2018) & (year(\"timestamp\") <= 2022))\n",
    "\n",
    "# Create a new column with just the year from the timestamp\n",
    "df_extracted_year = df_5years.withColumn(\"post_year\", year(\"timestamp\"))\n",
    "\n",
    "# Group by post_year and category and count the occurrences\n",
    "df_category_count = df_extracted_year.groupBy(\"post_year\", \"category\").agg(count(\"*\").alias(\"category_count\"))\n",
    "\n",
    "# Order the result for better readability\n",
    "df_result = df_category_count.orderBy(\"post_year\", \"category\", \"category_count\")\n",
    "\n",
    "display(df_result.select(\"*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1dd3ebd-a7f8-4755-bca8-27c8628961af",
     "showTitle": true,
     "title": "Query 3a) Identify the user with most followers in each country"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_geo = df_pin.join(df_geo, 'ind', 'inner')\n",
    "\n",
    "# Define window partition by country\n",
    "window_partition = Window.partitionBy(\"country\").orderBy(col(\"follower_count\").desc())\n",
    "\n",
    "# Use the window partition specification to add a row number for each user within each country partition\n",
    "df_ordered = df_pin_geo.withColumn(\"row_number\", row_number().over(window_partition))\n",
    "\n",
    "# Filter for user with higest follower_count (row_number 1) in each country\n",
    "df_user_highest_follower_each_country = df_ordered.filter(col(\"row_number\") == 1) \\\n",
    "                                   .select(\"country\", \"poster_name\", \"follower_count\")\n",
    "\n",
    "display(df_user_highest_follower_each_country.select(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36df6ba6-70eb-4bea-bef4-d07e60279cf1",
     "showTitle": true,
     "title": "Query 3b ) Identify the country with the user with most followers"
    }
   },
   "outputs": [],
   "source": [
    "# Identify the highest follower count across all country\n",
    "df_most_followers_all_countries = df_user_highest_follower_each_country.select(max(\"follower_count\")).collect()[0][0]\n",
    "\n",
    "# Find the country or countries with the user that has the highest global follower count\n",
    "df_country_with_highest_followers = df_user_highest_follower_each_country.select(\"*\").where(col(\"follower_count\") == df_most_followers_all_countries)\n",
    "\n",
    "# Display the results\n",
    "display(df_country_with_highest_followers.select(\"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56ef83c-a1f7-472a-b919-5a7523d23555",
     "showTitle": true,
     "title": "Query 4) Identify the most popular category for different age groups"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_user = df_pin.join(df_user, 'ind', 'inner')\n",
    "\n",
    "# Create the age_group categories\n",
    "df_with_age_categories = df_pin_user.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .otherwise(\"50+\")\n",
    ")\n",
    "\n",
    "# Define window partition specification by age_group and ordered by category_count, in descending order\n",
    "window_partition = Window.partitionBy(\"age_group\").orderBy(col(\"category_count\").desc())\n",
    "\n",
    "# Filter for the most popular category in each age group\n",
    "df_highest_category_per_age_group = df_with_age_categories.groupBy(\"age_group\", \"category\").agg(count(\"category\").alias(\"category_count\")) \\\n",
    "    .withColumn(\"rank\", row_number().over(window_partition)).filter(col(\"rank\") == 1).drop(\"rank\") \n",
    "\n",
    "display(df_highest_category_per_age_group)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f86ddc-fe4b-42a3-8a4a-b8d4308d317b",
     "showTitle": true,
     "title": "Query 5) Identify the median follower count for each age group"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_user = df_pin.join(df_user, 'ind', 'inner')\n",
    "\n",
    "# Create the age_group categories\n",
    "df_with_age_categories = df_pin_user.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .otherwise(\"50+\")\n",
    ")\n",
    "\n",
    "# Copmute the median follower count, for each age group\n",
    "df_follower_median = df_with_age_categories.groupBy(\"age_group\")\\\n",
    "                                            .agg(percentile_approx(\"follower_count\", 0.5).alias(\"median_follower_count\"))\n",
    "\n",
    "# Display the result\n",
    "display(df_follower_median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a523fde4-1222-4151-913c-acaeeea13398",
     "showTitle": true,
     "title": "Query 6) Find how many users joined each year (2015-2020)"
    }
   },
   "outputs": [],
   "source": [
    "# Parse date_joined from string to date type\n",
    "df_user_query = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(DateType()))\n",
    "\n",
    "# Extract the year from date_joined\n",
    "df_with_year = df_user_query.withColumn(\"post_year\", year(col(\"date_joined\")))\n",
    "\n",
    "# Filter years from 2015 to 2020\n",
    "df_6years = df_with_year.filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Count users, grouped by post_year\n",
    "df_users_per_year = df_6years.groupBy(\"post_year\").agg(count(\"*\").alias(\"number_users_joined\"))\n",
    "\n",
    "display(df_users_per_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60dd520-6153-49ef-985f-66a4ae98f3be",
     "showTitle": true,
     "title": "Query 7) Find the median follower count, based on the joining year of users who joined from 2015-2020"
    }
   },
   "outputs": [],
   "source": [
    "# Filter users who joined between 2015-2020\n",
    "df_users_6years = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\")) \\\n",
    "                           .withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    "                           .filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "df_users_6years_pin = df_users_6years.join(df_pin, 'ind', 'inner')\n",
    "\n",
    "# Calculate the median follower count per post year\n",
    "df_median_follower_count = df_users_6years_pin.groupBy(\"post_year\") \\\n",
    "                                    .agg(expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "display(df_median_follower_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae9cd9b-de4b-4eff-b3b7-99c012421aa0",
     "showTitle": true,
     "title": "Query 8) Find the median follower count of user based on their joining year and age group"
    }
   },
   "outputs": [],
   "source": [
    "# Filter users who joined between 2015-2020\n",
    "df_users_6years = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(\"timestamp\")) \\\n",
    "                           .withColumn(\"post_year\", year(\"date_joined\")) \\\n",
    "                           .filter((col(\"post_year\") >= 2015) & (col(\"post_year\") <= 2020))\n",
    "\n",
    "# Create the age_group categories\n",
    "df_with_age_categories = df_users_6years.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\").between(18, 24), \"18-24\")\n",
    "    .when(col(\"age\").between(25, 35), \"25-35\")\n",
    "    .when(col(\"age\").between(36, 50), \"36-50\")\n",
    "    .otherwise(\"50+\")\n",
    ")\n",
    "\n",
    "# merge df_users_age_grouped with df_pin on the user identifier to get follower counts\n",
    "df_with_age_categories_pin = df_with_age_categories.join(df_pin, 'ind', 'inner')\n",
    "\n",
    "# Group by age_group and post_year, and calculate the median follower count\n",
    "df_median_followers = df_with_age_categories_pin.groupBy(\"age_group\", \"post_year\") \\\n",
    "                               .agg(expr(\"percentile_approx(follower_count, 0.5)\").alias(\"median_follower_count\"))\n",
    "\n",
    "display(df_median_followers)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Batch_Data_Processing_and_Queries",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
