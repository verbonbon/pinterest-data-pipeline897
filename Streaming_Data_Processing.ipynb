{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577962b1-85c1-4351-9606-6238144aa103",
     "showTitle": true,
     "title": "Check contents in FileStore"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check the contents in FileStore\n",
    "dbutils.fs.ls(\"/FileStore/tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09828303-3d74-4ff5-b15e-a1f7ddc188b2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Retrieving Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b61ae51-29c7-4284-b1fd-e1c99f3f7b18",
     "showTitle": true,
     "title": "Step 1: Loading packages"
    }
   },
   "outputs": [],
   "source": [
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# URL processing\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee3b6660-092d-4961-9091-4313f70c52ed",
     "showTitle": true,
     "title": "Step 2: Read the csv file containing AWS keys to Databricks "
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "773e3b80-26e6-4e33-879e-33e9cee6924d",
     "showTitle": true,
     "title": "Step 3: Extract access key and secret access key from spark dataframe created above"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Notes: the secret access key will be encoded using urllib.parse.quote for security purposes. \n",
    "# safe=\"\" means that every character will be encoded.\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0505069-8704-489b-9913-9dd071b7379a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Read Streaming Data from Kinesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c73953-e4ec-4eea-aab7-9787e7d78a81",
     "showTitle": true,
     "title": "Create functions for creating dataframes and delta tables"
    }
   },
   "outputs": [],
   "source": [
    "# a function that retrieves Kienesis stream and returns a df\n",
    "def read_stream(stream_name: str):\n",
    "    df_stream_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format('kinesis') \\\n",
    "    .option('streamName', stream_name) \\\n",
    "    .option('initialPosition','earliest') \\\n",
    "    .option('region','us-east-1') \\\n",
    "    .option('awsAccessKey', ACCESS_KEY) \\\n",
    "    .option('awsSecretKey', SECRET_KEY) \\\n",
    "    .load()\n",
    "    return df_stream_raw\n",
    "    \n",
    "# a function that deserializes the data column of a dataframe to a string \n",
    "# then parse the JSON string into a structured format using a predefined schema\n",
    "# and returns a df\n",
    "def deserializer(stream, schema):\n",
    "    df_stream_raw = stream \\\n",
    "    .selectExpr(\"CAST(data as STRING) jsonData\") \\\n",
    "    .select(from_json(\"jsonData\", schema).alias(\"data\")).select(\"data.*\")\n",
    "    return df_stream_raw\n",
    "\n",
    "# a funcion that writes the cleaned df to delta table\n",
    "# the parameters are the df names and the topic name for insertion (i.e., pin, geo, user)\n",
    "def create_delta_table(df, topic_name: str):\n",
    "    df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "    .table(f\"0a5e6ec37a2f_{topic_name}_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5a6f16-d435-4f40-9d36-ece1c1861c96",
     "showTitle": true,
     "title": "Create Schemas to define variable type"
    }
   },
   "outputs": [],
   "source": [
    "# schema for pin data\n",
    "pin_schema = StructType([\n",
    "    StructField(\"index\", IntegerType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", IntegerType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "# schema for geo data\n",
    "geo_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"latitude\", FloatType()),\n",
    "    StructField(\"longitude\", FloatType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "# schema for user data\n",
    "user_schema = StructType([\n",
    "    StructField(\"ind\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", StringType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626cd50a-453d-4fe4-bb1c-53fb00bd8feb",
     "showTitle": true,
     "title": "Read the streams from Kinesis"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_stream_raw = read_stream('streaming-0a5e6ec37a2f-pin')\n",
    "df_geo_stream_raw = read_stream('streaming-0a5e6ec37a2f-geo')\n",
    "df_user_stream_raw = read_stream('streaming-0a5e6ec37a2f-user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05157c8-5bc8-4b25-830a-a789108875cf",
     "showTitle": true,
     "title": "Deserialize the streams"
    }
   },
   "outputs": [],
   "source": [
    "df_pin_stream_de = deserializer(df_pin_stream_raw, pin_schema)\n",
    "df_geo_stream_de = deserializer(df_geo_stream_raw, geo_schema)\n",
    "df_user_stream_de = deserializer(df_user_stream_raw, user_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80b8e6ed-2ed3-4e29-9979-3b225f9e65cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2b0d70-6238-463d-9ac0-b5719a8f5569",
     "showTitle": true,
     "title": "Cleaning pinterest dataframe (df_pin_stream_de)"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Drop duplicates\n",
    "df_pin= df_pin_stream_de.dropDuplicates()\n",
    "\n",
    "# Handle empty entires and entries with error\n",
    "# Replace them with \"None\"\n",
    "df_pin_ = df_pin.replace(['', ' ', 'NULL', 'null'], [None] * 4)\n",
    "df_pin = df_pin.withColumn(\"description\", when(col(\"description\") == \"No description available Story format\", None).otherwise(col(\"description\")))\n",
    "df_pin = df_pin.withColumn('follower_count', when(col('follower_count') == 'User Info Error', None).otherwise(col(\"follower_count\")))\n",
    "df_pin = df_pin.withColumn(\"image_src\", when(col(\"image_src\") == \"Image src error.\", None).otherwise(col(\"image_src\")))\n",
    "df_pin = df_pin.withColumn(\"poster_name\", when(col(\"poster_name\") == \"User Info Error\", None).otherwise(col(\"poster_name\")))\n",
    "df_pin = df_pin.withColumn(\"tag_list\", when(col(\"tag_list\") == \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", None).otherwise(col(\"tag_list\")))\n",
    "df_pin = df_pin_.withColumn(\"title\", when(col(\"title\") == \"No Title Data Available\", None).otherwise(col(\"title\")))\n",
    "\n",
    "# Transform follower_count to ensure every entry is a number\n",
    "# remove non-numeric entries, such that data type is \"int\"\n",
    "df_pin = df_pin.withColumn(\"follower_count\", regexp_replace(col(\"follower_count\"), \"[^0-9]\", \"\"))\n",
    "\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin = df_pin.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"Local save in \", \"\"))\n",
    "\n",
    "# Rename index to ind (to match the other dfs)\n",
    "df_pin = df_pin.withColumnRenamed(\"index\", \"ind\")\n",
    "df_pin = df_pin.withColumn('ind', df_pin['ind'].cast(IntegerType()))\n",
    "\n",
    "# Reorder df columns\n",
    "df_pin = df_pin.select([\"ind\", \"unique_id\", \"title\", \"description\", \"follower_count\", \"poster_name\", \"tag_list\", \"is_image_or_video\", \"image_src\", \"save_location\", \"category\"])\n",
    "\n",
    "#display(df_pin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f6260d-4579-4d3f-a34e-bfa95ed2fb72",
     "showTitle": true,
     "title": "Cleaning geo dataframe (df_geo_stream_de)"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_geo = df_geo_stream_de.dropDuplicates()\n",
    "\n",
    "# Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo = df_geo.drop(\"latitude\", \"longitude\")\n",
    "\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo = df_geo.withColumn(\"timestamp\", col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_geo = df_geo.select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    "\n",
    "display(df_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d17fbe2-3d3c-4d48-a085-9db9b454444c",
     "showTitle": true,
     "title": "Cleaning user dataframe (df_user_raw)"
    }
   },
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df_user = df_user_stream_de.dropDuplicates()\n",
    "\n",
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "df_user = df_user.drop(\"first_name\", \"last_name\")\n",
    "\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user = df_user.withColumn(\"date_joined\", col(\"date_joined\").cast(TimestampType()))\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df_user = df_user.select([\"ind\", \"user_name\", \"age\", \"date_joined\"])\n",
    "\n",
    "display(df_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b196836c-5b4a-4f72-af35-7d985c8efd87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f84b24-ab4d-4a9c-865b-46b80256b8d3",
     "showTitle": true,
     "title": "Pinterest Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "create_delta_table(df_pin, \"pin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ed67996-dbd7-4635-961c-8fa26a6ee708",
     "showTitle": true,
     "title": "Geo Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "create_delta_table(df_geo, \"geo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623e587f-3d3e-48f0-aa50-fb15cf007b2f",
     "showTitle": true,
     "title": "User Delta Table"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "create_delta_table(df_user, \"user\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Streaming_Data_Processing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
